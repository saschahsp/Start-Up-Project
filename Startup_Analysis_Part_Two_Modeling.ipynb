{"cells":[{"cell_type":"markdown","source":["<p style=\"font-size: 100px; text-align: center; color: rgb(212, 69, 0)\"> <strong>S</strong>tartup <strong>A</strong>nalysis </p>"],"metadata":{}},{"cell_type":"markdown","source":["<p style=\"font-size: 60px; text-align: center; color: rgb(62, 61, 60)\"> <strong>P</strong>art <strong>T</strong>wo</p>"],"metadata":{}},{"cell_type":"markdown","source":["<p style=\"text-align: center\"><img src=\"https://i.vimeocdn.com/portrait/4910448_300x300\" style=\"height: 50%; text-align: center\"/></p>"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\n# Load functionality to manipulate dataframes\nfrom pyspark.sql import functions as fn\nimport matplotlib.pyplot as plt\nfrom pyspark.sql.functions import stddev, mean, col\n# Functionality for computing features\nfrom pyspark.ml import feature, regression, classification, Pipeline\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import feature, regression, classification, Pipeline\nfrom pyspark.ml.feature import Tokenizer, VectorAssembler, HashingTF, Word2Vec, StringIndexer, OneHotEncoder\nfrom pyspark.ml import clustering\nfrom itertools import chain\nfrom pyspark.ml.linalg import Vectors, VectorUDT\nfrom pyspark.ml import classification\nfrom pyspark.ml.classification import LogisticRegression, RandomForestClassifier\nfrom pyspark.ml import evaluation\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark import keyword_only\nfrom pyspark.ml import Transformer\nfrom pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["#LOAD AND IMPORT"],"metadata":{}},{"cell_type":"code","source":["%sh wget https://www.dropbox.com/s/zj133oas3xa1mma/master.csv -nv"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# load master dataset\ndfmaster = sqlContext.read.format(\"csv\").load(\"file:///databricks/driver/master.csv\", delimiter = \",\", header = True)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["# Preparations and Understanding before Modeling"],"metadata":{}},{"cell_type":"code","source":["# create a 0/1 column for acquistions\ndfmaster = dfmaster.\\\n  withColumn(\"labelacq\", fn.when(col(\"status\") == \"acquired\",\"1\").otherwise(\"0\"))"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# number of rows in master table\nprint(dfmaster.count())"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["### NAs and market column (with too many levels) handeling"],"metadata":{}},{"cell_type":"code","source":["# check for missing values \ndfmaster.toPandas().isnull().sum()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# drop market columns because of too many level and better breakdown with the category_final column\ndfmaster1 = dfmaster.drop(\"market\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# drop rows with missing values\ndfmaster1drop = dfmaster1.na.drop()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["We decided to drop the rows with any NULL value, because we still had enough observations and we did not want to value the missing entries equally for every observation. We did one testing and the AUC performance was worse anyways. However, for further analysis it would be worth to try different methods to fill in the missing values."],"metadata":{}},{"cell_type":"markdown","source":["### String indexer, one hot encoder and casting to numerics"],"metadata":{}},{"cell_type":"code","source":["# create index for categorical variables\n# use pipline to apply indexer\nlist1 = [\"country_code\",\"city\",\"quarter_new\",\"investor_country_code\",\"funding_round_type\",\"category_final\"]\nindexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(dfmaster1drop) for column in list1]\npipelineindex = Pipeline(stages=indexers).fit(dfmaster1drop)\ndfmasternew = pipelineindex.transform(dfmaster1drop)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# convert string to double for numerical variables\ndfmasternew = dfmasternew.\\\n  withColumn(\"numeric_funding_rounds\", dfmasternew[\"funding_rounds\"].cast(\"double\")).\\\n  withColumn(\"numeric_age\", dfmasternew[\"age\"].cast(\"double\")).\\\n  withColumn(\"numeric_count_investor\", dfmasternew[\"count_investor\"].cast(\"double\")).\\\n  withColumn(\"numeric_time_to_first_funding\", dfmasternew[\"time_to_first_funding\"].cast(\"double\")).\\\n  withColumn(\"numeric_total_raised_usd\", dfmasternew[\"total_raised_usd\"].cast(\"double\")).\\\n  withColumn(\"label\", dfmasternew[\"labelacq\"].cast(\"double\"))"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# save\ndfone = dfmasternew"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["#display(dfone)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# list of index columns of categorical variables for the onehotencoder\nlist2 = dfone.columns[16:22]\nlist2"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["# create sparse matrix of indexed categorical columns\n# use pipline to apply the encoder\nonehotencoder_stages = [OneHotEncoder(inputCol=c, outputCol='onehotencoded_' + c) for c in list2]\npipelineonehot = Pipeline(stages=onehotencoder_stages)\npipeline_mode = pipelineonehot.fit(dfone)\ndf_coded = pipeline_mode.transform(dfone)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["display(df_coded)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Data split, defining vector assemblers & standard scaler and creating labellist"],"metadata":{}},{"cell_type":"code","source":["# split dataset into training, validaiton and testing dataset\ntraining_df, validation_df, testing_df = df_coded.randomSplit([0.6, 0.3, 0.1])"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["VECTOR ASSEMBLER"],"metadata":{}},{"cell_type":"code","source":["training_df.columns[22:27]"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["training_df.columns[28:37]"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["We chose to only standardize the numeric values, because of interpretability. \n\nFurther, our explained variance when standardizing all of the columns was very strange, because it was very low as seen in the figure below. However, the testing performance was the same if not a little bit better and the weights (for Logistric Regression) were better in terms of interpretation - you can take a look at it in the additional analysis section at the end of the notebook.  \n\n**Explained Variance with all columns standardized**\n\n<img src=\"https://i.imgur.com/kpjILwW.png\"/>\n\n**Explained Variance with only numerical columns standardized**\n\n<img src=\"https://i.imgur.com/2xKpi46.png\"/>"],"metadata":{}},{"cell_type":"code","source":["# define vector assembler with the features for the modelling\nvanum = VectorAssembler(). \\\n      setInputCols(training_df.columns[22:27]). \\\n      setOutputCol('features_nonstd')"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["# define vector assembler with the features for the modelling\nvacate = VectorAssembler(). \\\n      setInputCols(training_df.columns[28:37]). \\\n      setOutputCol('featurescate')"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["va = VectorAssembler(). \\\n      setInputCols(['featuresnum','featurescate']). \\\n      setOutputCol('features')"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["std = feature.StandardScaler(withMean=True, withStd=True).setInputCol('features_nonstd').setOutputCol('featuresnum')"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["#display(va.transform(training_df))"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# suffix for investor country code because intersection with county_code of  the companies\ninvcc = ['{}_{}'.format(a, \"investor\") for a in indexers[3].labels]"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# define labellist by using the indexer stages for displaying the weights & loadings\nlabellist = training_df.columns[22:27] + indexers[0].labels + indexers[1].labels + indexers[2].labels + invcc + indexers[4].labels + indexers[5].labels"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["training_df.columns[28:37]"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# null dummy for onehotencoded_country_code_index\nprint(\"null dummy for onehotencoded_country_code_index\")\nprint(len(indexers[0].labels))\nprint(indexers[0].labels[79])\n# null dummy for onehotencoded_city_index\nprint(\"null dummy for onehotencoded_city_index\")\nprint(indexers[1].labels[1761])\nprint(len(indexers[1].labels))\n# null dummy for onehotencoded_quarter_new_index\nprint(\"null dummy for onehotencoded_quarter_new_index\")\nprint(len(indexers[2].labels))\nprint(indexers[2].labels[3])\n# null dummy for onehotencoded_investor_country_code_index\nprint(\"null dummy for onehotencoded_investor_country_code_index\")\nprint(len(invcc))\nprint(invcc[67])\n# null dummy for onehotencoded_funding_round_type_index\nprint(\"null dummy for onehotencoded_funding_round_type_index\")\nprint(len(indexers[4].labels))\nprint(indexers[4].labels[12])\n# null dummy for onehotencoded_category_final_index\nprint(\"null dummy for onehotencoded_category_final_index\")\nprint(len(indexers[5].labels))\nprint(indexers[5].labels[210])"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# delete for null dummys from labellist\nlabellist.remove(\"SRB\")\nlabellist.remove(\"Aldermaston\")\nlabellist.remove(\"Q4\")\nlabellist.remove(\"SVK_investor\")\nlabellist.remove(\"secondary_market\")\nlabellist.remove(\"Video\")"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["# Modeling"],"metadata":{}},{"cell_type":"markdown","source":["## RANDOM FOREST"],"metadata":{}},{"cell_type":"code","source":["# define binary classification evaluator\nbce = BinaryClassificationEvaluator()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["We chose to run Random Forest models with 20, 15 and 25 trees."],"metadata":{}},{"cell_type":"code","source":["# define default, 15 trees and 25 trees random forest classifier\nrf = RandomForestClassifier(maxBins=10000, featuresCol='features', labelCol='label')\nrf15 = RandomForestClassifier(numTrees=15, maxBins=10000, featuresCol='features', labelCol='label')\nrf25 = RandomForestClassifier(numTrees=25, maxBins=10000, featuresCol='features', labelCol='label')"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# define and fit pipelines with vector assembler and random forest classifier \nrf_pipeline = Pipeline(stages=[vanum, std, vacate, va, rf]).fit(training_df)\nrf_pipeline_15 = Pipeline(stages=[vanum, std, vacate, va, rf15]).fit(training_df)\nrf_pipeline_25 = Pipeline(stages=[vanum, std, vacate, va, rf25]).fit(training_df)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["# apply pipeline to validiaton dataset\ndfrf = rf_pipeline.transform(validation_df)\ndfrf_15 = rf_pipeline_15.transform(validation_df)\ndfrf_25 = rf_pipeline_25.transform(validation_df)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["### Performance"],"metadata":{}},{"cell_type":"code","source":["# print the areas under the curve for the different random forest pipelines\nprint(\"Random Forest with 20 trees: AUC = {}\".format(bce.evaluate(dfrf)))\nprint(\"Random Forest 15 trees: AUC = {}\".format(bce.evaluate(dfrf_15)))\nprint(\"Random Forest 25 trees: AUC = {}\".format(bce.evaluate(dfrf_25)))"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["The performance in terms of AUC is very much the same for all three models. We always also look at the AUC, because our target variable is not really well balanced. AUC will give us the between the TRUE POSITIVE and FALSE POSITIVE rate. The accuracy on the valdidation dataset seems very good. However, it is strange that the accuracies are exactly the same even though the raw predicitions and probabilities are different. The AUC is harder to estimate, because there is no general rule which value is good - 0.71 seems okay though."],"metadata":{}},{"cell_type":"code","source":["# print the accuracies for the different random forest pipelines\nprint(dfrf.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Random Forest with 20 trees\")).show())\nprint(dfrf_15.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Random Forest with 15 trees\")).show())\nprint(dfrf_25.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Random Forest with 25 trees\")).show())"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["For some reason the accuracy is exactly the same for all three models, probably meaning that the three models do not have a significant difference. However, the values for the importancies (see down below) are different."],"metadata":{}},{"cell_type":"markdown","source":["### Importancies"],"metadata":{}},{"cell_type":"code","source":["# create spark df with the 20 highest labels and the corresponding importancies + sorting by importancy\nrfw = spark.createDataFrame(pd.DataFrame(list(zip(labellist, rf_pipeline.stages[4].featureImportances.toArray())),\n            columns = ['column', 'importancy']).sort_values('importancy').tail(20))\ndisplay(rfw)"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["# create spark df with the labels and the corresponding importancies + sorting by importancy\nrf15w = spark.createDataFrame(pd.DataFrame(list(zip(labellist, rf_pipeline_15.stages[4].featureImportances.toArray())),\n            columns = ['column', 'importancy']).sort_values('importancy').tail(20))\ndisplay(rf15w)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["# create spark df with the labels and the corresponding importancies + sorting by importancy\nrf25w = spark.createDataFrame(pd.DataFrame(list(zip(labellist, rf_pipeline_25.stages[4].featureImportances.toArray())),\n            columns = ['column', 'weight']).sort_values('weight').tail(20))\ndisplay(rf25w)"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["The dataframe above displays the most contributing factors for the prediction of an acquistion. However, we can not say if the impact is negativ or positiv. We used Logistic Regression to explain the direction of the feature. \n\n<img src=\"https://i.imgur.com/RH5DaS3.png\"/>\n\n*those were the important features for the best performing RF with the tranining dataset at that time*\n\nThe calculation for this figure is in the Logistic Regression part.\n\nThe table shows us that a company that is established, is located in the USA, has the majority of investors from the USA and has venture funding and a decent investore count has a good chance to get acquired. Also, the storage market seems to be hot (category)"],"metadata":{}},{"cell_type":"markdown","source":["## LOGISTIC REGRESSION"],"metadata":{}},{"cell_type":"code","source":["# define logistic regression parameters and pipeline stages \nlambda_par1 = 0.01\nalpha_par1 = 0.05\nen_lr1 = LogisticRegression().\\\n        setLabelCol('label').\\\n        setFeaturesCol('features').\\\n        setRegParam(lambda_par1).\\\n        setElasticNetParam(alpha_par1)\n\n# change the parameters of the second classifier below\nlambda_par2 = 0.05\nalpha_par2 = 0.05\nen_lr2 = LogisticRegression().\\\n        setLabelCol('label').\\\n        setFeaturesCol('features').\\\n        setRegParam(lambda_par2).\\\n        setElasticNetParam(alpha_par2)\n\n# change the parameters of the thrid classifier below\nlambda_par3 = 0.1\nalpha_par3 = 0.05\nen_lr3 = LogisticRegression().\\\n        setLabelCol('label').\\\n        setFeaturesCol('features').\\\n        setRegParam(lambda_par3).\\\n        setElasticNetParam(alpha_par3)\n        \nen_lr_estimator1 = Pipeline(\n    stages=[vanum, std, vacate, va, en_lr1])\nen_lr_estimator2 = Pipeline(\n    stages=[vanum, std, vacate, va, en_lr2])\nen_lr_estimator3 = Pipeline(\n    stages=[vanum, std, vacate, va, en_lr3])"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["*A function to possibly search for the optimal lambda and alpha*"],"metadata":{}},{"cell_type":"code","source":["#lstauc = []\n#lsacc = []\n#lsi = []\n#lsa = []\n#for i in range(20):\n#  lambda_par3 = 0.01 + i*0.01\n#  for a in range(20):\n#    alpha_par3 = 0.01 + i*0.01\n#    en_lr3 = LogisticRegression().\\\n#    setLabelCol('label').\\\n#    setFeaturesCol('features').\\\n#    setRegParam(lambda_par3).\\\n#    setElasticNetParam(alpha_par3)\n#    en_lr_estimator3 = Pipeline(stages=[va, en_lr3])\n#    en_lr_model3 = en_lr_estimator3.fit(training_df)\n#    dfmodel3 = en_lr_model3.transform(validation_df)\n#    auc = bce.evaluate(dfmodel3)\n#    lstauc.append(auc)\n#    acc = dfmodel3.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"acc\")).toPandas().acc[0]\n#    lsacc.append(acc)\n#    #lsi.append[i]\n#    #lsa.append[a]"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"code","source":["# fit logistic regression pipelines\nen_lr_model1 = en_lr_estimator1.fit(training_df)\nen_lr_model2 = en_lr_estimator2.fit(training_df)\nen_lr_model3 = en_lr_estimator3.fit(training_df)"],"metadata":{},"outputs":[],"execution_count":61},{"cell_type":"code","source":["# apply pipeline to validation dataset\ndfmodel1 = en_lr_model1.transform(validation_df)\ndfmodel2 = en_lr_model2.transform(validation_df)\ndfmodel3 = en_lr_model3.transform(validation_df)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["### Performance"],"metadata":{}},{"cell_type":"code","source":["# print areas under the curve of the different logistic regressions\nprint(\"Logistic Regression Model 1: AUC = {}\".format(bce.evaluate(dfmodel1)))\nprint(\"Logistic Regression Model 2: AUC = {}\".format(bce.evaluate(dfmodel2)))\nprint(\"Logistic Regression Model 3: AUC = {}\".format(bce.evaluate(dfmodel3)))"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["The AUC seems to be better in comparison to Random Forest, while the accuracy is a little bitter lower or almost the same. \n\nLogistic regressions seems to perform a litter bit better than Random Forest since the AUC is higher."],"metadata":{}},{"cell_type":"code","source":["# print accuracies of the different logistic regressions\nprint(dfmodel1.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Model 1\")).show())\nprint(dfmodel2.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Model 2\")).show())\nprint(dfmodel3.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Model 3\")).show())"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["### Weights"],"metadata":{}},{"cell_type":"markdown","source":["The weights are almost useless, because the level of categories seems to be too high in some features and therefore, the weights are close to each other and we also have high amount of features with weight 0 (38%). We just use the weights as a reference for the importancy of Random Forest, as already described."],"metadata":{}},{"cell_type":"code","source":["\ndisplay(spark.createDataFrame(pd.DataFrame(list(zip(labellist, en_lr_model1.stages[4].coefficients.toArray())),\n            columns = ['label', 'weight']).sort_values('weight')).agg((fn.count(fn.when(col(\"weight\") == 0, True)) / fn.count(col(\"label\"))).alias(\"Ratio\")))"],"metadata":{},"outputs":[],"execution_count":69},{"cell_type":"code","source":["# create spark df with the first 10 lowest labels and corresponding weights\npd.DataFrame(list(zip(labellist, en_lr_model1.stages[4].coefficients.toArray())),\n            columns = ['label', 'weight']).sort_values('weight').head(10)"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["# create spark df with the first 10 highest labels and corresponding weights\npd.DataFrame(list(zip(labellist, en_lr_model1.stages[4].coefficients.toArray())),\n            columns = ['label', 'weight']).sort_values('weight').tail(10)"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["# create spark df with the first 10 lowest labels and corresponding weights\npd.DataFrame(list(zip(labellist, en_lr_model2.stages[4].coefficients.toArray())),\n            columns = ['label', 'weight']).sort_values('weight').head(10)"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["# create spark df with the first 10 highest labels and corresponding weights\npd.DataFrame(list(zip(labellist, en_lr_model2.stages[4].coefficients.toArray())),\n            columns = ['label', 'weight']).sort_values('weight').tail(10)"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"code","source":["# create spark df with the first lowest labels and corresponding weights\npd.DataFrame(list(zip(labellist, en_lr_model3.stages[4].coefficients.toArray())),\n            columns = ['label', 'weight']).sort_values('weight').head(10)"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"code","source":["# create spark df with the first 10 highest labels and corresponding weights\npd.DataFrame(list(zip(labellist, en_lr_model3.stages[4].coefficients.toArray())),\n            columns = ['label', 'weight']).sort_values('weight').tail(10)"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":["### Weights of the important features of the RF with 15 trees as reference"],"metadata":{}},{"cell_type":"code","source":["# create spark df with all coefficients of LR model 2 (best performance in terms of AUC and accuracy combined)\ndflrcoeff = spark.createDataFrame(pd.DataFrame(list(zip(labellist, en_lr_model2.stages[4].coefficients.toArray())),\n            columns = ['label', 'weight']).sort_values('weight'))"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["# list with the highest 8 importancies based on RF model with 15 trees (best performance in terms of AUC and accuracy combined)\nrf15imp = [\"Storage\",\"Messaging\",\"numeric_count_investor\",\"USA\",\"venture\",\"USA_investor\",\"numeric_age\",\"numeric_total_raised_usd\"]"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"code","source":["# display weights for the important factors from the RF model with 15 trees in order to see if its negative or positive\ndisplay(dflrcoeff.where(col(\"label\").isin(rf15imp)).orderBy(\"weight\"))"],"metadata":{},"outputs":[],"execution_count":79},{"cell_type":"markdown","source":["## NEURAL NETWORKS"],"metadata":{}},{"cell_type":"markdown","source":["### Standard Model"],"metadata":{}},{"cell_type":"code","source":["# define neural networks (MultilayerPerceptron) classifier\nmlp = classification.MultilayerPerceptronClassifier(seed=0).\\\n    setStepSize(0.2).\\\n    setMaxIter(200).\\\n    setLayers([2137, 2]).\\\n    setFeaturesCol('features')"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["# define and fit neural network pipeline \nmlp_simple_model = Pipeline(stages=[vanum, std, vacate, va, mlp]).fit(training_df)"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["# define evaluators for accuracy and area under the curve\nevaluator = evaluation.MulticlassClassificationEvaluator(metricName=\"accuracy\")\nevaluatorauc = evaluation.MulticlassClassificationEvaluator()"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["# apply pipeline to validation dataset\ndfnn = mlp_simple_model.transform(validation_df)"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["# print accuracy and area under the curve for NN\nprint(evaluator.evaluate(dfnn))\nprint(evaluatorauc.evaluate(dfnn))"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"markdown","source":["### Models with Hidden Layers"],"metadata":{}},{"cell_type":"code","source":["# define neural networks (MultilayerPerceptron) classifier with hidden layers\nmlp2 = classification.MultilayerPerceptronClassifier(seed=0).\\\n    setStepSize(0.2).\\\n    setMaxIter(200).\\\n    setFeaturesCol('features').\\\n    setLayers([2137,30,30, 2])\nmlp3 = classification.MultilayerPerceptronClassifier(seed=0).\\\n    setStepSize(0.2).\\\n    setMaxIter(200).\\\n    setFeaturesCol('features').\\\n    setLayers([2137,10,10, 2])\nmlp4 = classification.MultilayerPerceptronClassifier(seed=0).\\\n    setStepSize(0.2).\\\n    setMaxIter(200).\\\n    setFeaturesCol('features').\\\n    setLayers([2137,20,20, 2])\nmlp5 = classification.MultilayerPerceptronClassifier(seed=0).\\\n    setStepSize(0.2).\\\n    setMaxIter(200).\\\n    setFeaturesCol('features').\\\n    setLayers([2137,30, 2])\nmlp6 = classification.MultilayerPerceptronClassifier(seed=0).\\\n    setStepSize(0.2).\\\n    setMaxIter(200).\\\n    setFeaturesCol('features').\\\n    setLayers([2137,30,30,30, 2])"],"metadata":{},"outputs":[],"execution_count":88},{"cell_type":"code","source":["# define and fit pipeline\nmlp2_model = Pipeline(stages=[vanum, std, vacate, va, mlp2]).fit(training_df)\nmlp3_model = Pipeline(stages=[vanum, std, vacate, va, mlp3]).fit(training_df)\nmlp4_model = Pipeline(stages=[vanum, std, vacate, va, mlp4]).fit(training_df)\nmlp5_model = Pipeline(stages=[vanum, std, vacate, va, mlp3]).fit(training_df)\nmlp6_model = Pipeline(stages=[vanum, std, vacate, va, mlp4]).fit(training_df)"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["# apply and fit model to validation dataset\ndfnn2 = mlp2_model.transform(validation_df)\ndfnn3 = mlp3_model.transform(validation_df)\ndfnn4 = mlp4_model.transform(validation_df)\ndfnn5 = mlp3_model.transform(validation_df)\ndfnn6 = mlp4_model.transform(validation_df)"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"markdown","source":["### Performance"],"metadata":{}},{"cell_type":"markdown","source":["The accuracy of the Neural Networks models is a bit worse in comparison to Random Forest and Logistic Regression (on the validation dataset). However, the AUC increases by a lot. This actually might be an indication that the Neural Networks performs the best, regardless of the worse accuracy.\n\nNN Model with 2 hidden layers and 30 neurons each seems to perform the best."],"metadata":{}},{"cell_type":"code","source":["# print accuracy and area under the curve\nprint(\"NN Model with 2 hidden layers and 30 neurons each: Accuracy = {}\".format(evaluator.evaluate(dfnn2)))\nprint(\"NN Model with 2 hidden layers and 30 neurons each: AUC = {}\".format(evaluatorauc.evaluate(dfnn2)))\nprint(\"____________________________________________________________________________\")\nprint(\"NN Model with 2 hidden layers and 10 neurons each: Accuracy = {}\".format(evaluator.evaluate(dfnn3)))\nprint(\"NN Model with 2 hidden layers and 10 neurons each: AUC = {}\".format(evaluatorauc.evaluate(dfnn3)))\nprint(\"____________________________________________________________________________\")\nprint(\"NN Model with 2 hidden layers and 20 neurons each: Accuracy = {}\".format(evaluator.evaluate(dfnn4)))\nprint(\"NN Model with 2 hidden layers and 20 neurons each: AUC = {}\".format(evaluatorauc.evaluate(dfnn4)))\nprint(\"____________________________________________________________________________\")\nprint(\"NN Model with 1 hidden layer and 30 neurons: Accuracy = {}\".format(evaluator.evaluate(dfnn5)))\nprint(\"NN Model with 1 hidden layer and 30 neurons: AUC = {}\".format(evaluatorauc.evaluate(dfnn5)))\nprint(\"____________________________________________________________________________\")\nprint(\"NN Model with 3 hidden layers and 30 neurons each: Accuracy = {}\".format(evaluator.evaluate(dfnn6)))\nprint(\"NN Model with 3 hidden layers and 30 neurons each: AUC = {}\".format(evaluatorauc.evaluate(dfnn6)))"],"metadata":{},"outputs":[],"execution_count":93},{"cell_type":"markdown","source":["##PCA"],"metadata":{}},{"cell_type":"markdown","source":["### Decision of the right Number of PCs"],"metadata":{}},{"cell_type":"markdown","source":["We also performed a dimensional reduction due to the high number of feature, which is mainly caused by the levels of categorical features. We chose to use 200 PCs as a base and then to look at the cumulative variance. We found out that roughly 85 PCs can explain 90% of the variance."],"metadata":{}},{"cell_type":"code","source":["# define and fit a PCA model wit k = 200 as reference\n# \npcavar = feature.PCA(k=200, inputCol='features', outputCol='pca_feat')\npca_var = Pipeline(\n      stages=[vanum, std, vacate, va, pcavar])\npca_var_model = pca_var.fit(df_coded)\nvarlist = pca_var_model.stages[4].explainedVariance\nnpvar = np.cumsum(varlist)\npci = [pci for pci in range(1,201,1)]\ndfvar = spark.createDataFrame(pd.DataFrame({\"Number of PCs\": pci, \"Cumulative Variance Explained\": npvar}))\ndisplay(dfvar)"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"markdown","source":["**So we reach a explained variance of 90% with roughly 85 PCs**"],"metadata":{}},{"cell_type":"markdown","source":["### Modeling"],"metadata":{}},{"cell_type":"code","source":["# define pca feature function with k = 85\npca = feature.PCA(k=85, inputCol='features', outputCol='pca_feat')"],"metadata":{},"outputs":[],"execution_count":100},{"cell_type":"code","source":["# define the parameters and pipelines for the logistic regressions with PCA features\nlambda_par1 = 0.01\nalpha_par1 = 0.05\nen_lr1_pca = LogisticRegression().\\\n        setLabelCol('label').\\\n        setFeaturesCol('pca_feat').\\\n        setRegParam(lambda_par1).\\\n        setElasticNetParam(alpha_par1)\n\n# change the parameters of the second classifier below\nlambda_par2 = 0.05\nalpha_par2 = 0.05\nen_lr2_pca = LogisticRegression().\\\n        setLabelCol('label').\\\n        setFeaturesCol('pca_feat').\\\n        setRegParam(lambda_par2).\\\n        setElasticNetParam(alpha_par2)\n\n# change the parameters of the thrid classifier below\nlambda_par3 = 0.1\nalpha_par3 = 0.05\nen_lr3_pca = LogisticRegression().\\\n        setLabelCol('label').\\\n        setFeaturesCol('pca_feat').\\\n        setRegParam(lambda_par3).\\\n        setElasticNetParam(alpha_par3)\n        \nen_lr_estimator1_pca = Pipeline(\n    stages=[vanum, std, vacate, va, pca, en_lr1])\nen_lr_estimator2_pca = Pipeline(\n    stages=[vanum, std, vacate, va, pca, en_lr2])\nen_lr_estimator3_pca = Pipeline(\n    stages=[vanum, std, vacate, va, pca, en_lr3])"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["# fit the PCA logistic regression pipelines\nen_lr_model1_pca = en_lr_estimator1_pca.fit(training_df)\nen_lr_model2_pca = en_lr_estimator2_pca.fit(training_df)\nen_lr_model3_pca = en_lr_estimator3_pca.fit(training_df)"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["# apply the pipelines to the validation dataset\ndfmodel1_pca = en_lr_model1_pca.transform(validation_df)\ndfmodel2_pca = en_lr_model2_pca.transform(validation_df)\ndfmodel3_pca = en_lr_model3_pca.transform(validation_df)"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["#display(dfmodel2_pca)"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"markdown","source":["### Performance"],"metadata":{}},{"cell_type":"markdown","source":["For some, the numbers of the AUC and accuracies are the same as of the Logistic Regression. The same applies to the raw predictions and probabilities. This might mean that the dimensional reduction has basically no impact at all.\n\nSo the performance of the Logistic Regression with PCA has the same conclusion as the normla Logisic Regression.\n\n*We feel like there is a mistake, but we can not find it*"],"metadata":{}},{"cell_type":"code","source":["display(dfmodel2_pca)"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["display(dfmodel2)"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["# print the areas under the curve and accuracies of the different PCA logistic regression models\nprint(\"Logistic Regression Model 1: AUC = {}\".format(bce.evaluate(dfmodel1_pca)))\nprint(\"Logistic Regression Model 2: AUC = {}\".format(bce.evaluate(dfmodel2_pca)))\nprint(\"Logistic Regression Model 3: AUC = {}\".format(bce.evaluate(dfmodel3_pca)))\n#\nprint(dfmodel1_pca.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Model 1\")).show())\nprint(dfmodel2_pca.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Model 2\")).show())\nprint(dfmodel3_pca.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Model 3\")).show())"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"markdown","source":["### Exploring Loadings"],"metadata":{}},{"cell_type":"code","source":["# assign loadings of the first 3 PCs \npc1 = en_lr_model2_pca.stages[4].pc.toArray()[:, 0].tolist()\npc2 = en_lr_model2_pca.stages[4].pc.toArray()[:, 1].tolist()\npc3 = en_lr_model2_pca.stages[4].pc.toArray()[:, 2].tolist()"],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"code","source":["# create pandas df with the loadings/PCs and the corresponding labels\npc_loadings = pd.DataFrame([labellist, pc1, pc2, pc3]).T.rename(columns={0: 'label', \n                                                                          1: 'load_pc1',\n                                                                          2: 'load_pc2',\n                                                                          3: 'load_pc3',})"],"metadata":{},"outputs":[],"execution_count":112},{"cell_type":"code","source":["# create spark df with the 5 highest and lowest loadings of the first PC\nload1 = spark.createDataFrame(pd.concat((pc_loadings.sort_values('load_pc1').head(), \n           pc_loadings.sort_values('load_pc1').tail())))\ndisplay(load1)"],"metadata":{},"outputs":[],"execution_count":113},{"cell_type":"markdown","source":["Here we can see the top and bottom 5 features based on the loadings that describe the first PC."],"metadata":{}},{"cell_type":"code","source":["# create spark df with the 5 highest and lowest loadings of the second PC\nload2 = spark.createDataFrame(pd.concat((pc_loadings.sort_values('load_pc2').head(), \n           pc_loadings.sort_values('load_pc2').tail())))\ndisplay(load2)"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"markdown","source":["Here we can see the top and bottom 5 features based on the loadings that describe the second PC."],"metadata":{}},{"cell_type":"code","source":["# create spark df with the 5 highest and lowest loadings of the third PC\nload3 = spark.createDataFrame(pd.concat((pc_loadings.sort_values('load_pc3').head(), \n           pc_loadings.sort_values('load_pc3').tail())))\ndisplay(load3)"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"markdown","source":["Here we can see the top and bottom 5 features based on the loadings that describe the third PC."],"metadata":{}},{"cell_type":"markdown","source":["### Plotting the first two PCs (with our own method)"],"metadata":{}},{"cell_type":"code","source":["# create spark df with the loadings of the first two PCs\ndfpc_loadings = (spark.createDataFrame(pc_loadings))"],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"code","source":["dfmodel2_pca_pd = dfmodel2_pca.toPandas()"],"metadata":{},"outputs":[],"execution_count":121},{"cell_type":"code","source":["# loadings of PC1 as numpy array\nnppc1 = np.asarray(pc1, dtype=np.float32)\n# loadings of PC2 as numpy array\nnppc2 = np.asarray(pc2, dtype=np.float32)\n\npc1list = []\npc2list = []\nlabellist2 = []\n\n# calculate PC1 and PC2 for the first 500 rows\nfor i in range(500):    \n  feat = dfmodel2_pca_pd.features[i].toArray()\n  multi1 = np.multiply(nppc1,feat)\n  multi2 = np.multiply(nppc2,feat)\n  pc1list.append(sum(multi1))\n  pc2list.append(sum(multi2))\n  labellist2.append(dfmodel2_pca_pd.labelacq[i])\n# the pc lists represent basically the pca_feat column after the PCA transformation"],"metadata":{},"outputs":[],"execution_count":122},{"cell_type":"code","source":["dfpcas = spark.createDataFrame(pd.DataFrame({\"Acquisition\": labellist2, \"PC1\": pc1list, \"PC2\": pc2list}))"],"metadata":{},"outputs":[],"execution_count":123},{"cell_type":"markdown","source":["Down below, we plotted the PC1 and PC2 o the first 500 observations. The color indicates if it was an acquisition or not. \n\n(*for this traning dataset*) The first PC is described by age & time to first funding (+) and number of investors & funding rounds (-). The second PC is described strongly by seed (+) and funding rounds & number of investors (-), and to a lesser degree age & total raised in USD. \n\nThe figure has a cone like shape with more acquistions towards the middle of the cone. The majority of the observations gather around 1 - 0.5 (PC2) and -2 - 0 (PC1). This might indicate that most of the companies got seeded (PC2), are young, had a short time to first funding, has a good amount of investors and/or funding rounds. \n\nThe companies that open op the cone to the right side seems to be older and had a longer time to first funding, as well as probably no seed funding. The cone opening on the left side might be younger than the average company, had a fast first funding and seemed to have more funding rounds & number of investors than the average company.\n\nIt is hard to see a pattern for the acquistions, so it would require more analyses (or the existing analyses as a support) to make any comments about the position of the acquistions."],"metadata":{}},{"cell_type":"code","source":["display(dfpcas)"],"metadata":{},"outputs":[],"execution_count":125},{"cell_type":"markdown","source":["### Loadings vs. weights"],"metadata":{}},{"cell_type":"code","source":["# create spark df with all coefficients of LR model 2 (best performance in terms of AUC and accuracy combined)\n# dflrcoeff\nintloadings = [\"seed\",\"Q1\",\"Q2\",\"Q3\",\"GBR_investor\",\"GBR\",\"numeric_time_to_first_funding\",\"numeric_age\",\"numeric_total_raised_usd\",\"numeric_count_investor\",\"numeric_funding_rounds\",\"venture\",\"USA_investor\"]\n\ndflrcoeff_filtered = dflrcoeff.where(col(\"label\").isin(intloadings)).orderBy(col(\"weight\").desc())\ndisplay(dflrcoeff_filtered)"],"metadata":{},"outputs":[],"execution_count":127},{"cell_type":"markdown","source":["We also tried to compare the loadings with the weights of the features in order to get any kind of insight. \n\nMaybe it would be possible to bring the loadings and weights of the features on the same scale and then create some sort of likelihood areas in different PC plots. And then its maybe even possible to combine these plots in order to make a prediction. \n\n*But that would be a project on its own*"],"metadata":{}},{"cell_type":"code","source":["# Display loadings and corresponding weights from the Logistic Regression\nload1drop = load1.drop(\"load_pc2\",\"load_pc3\").orderBy(col(\"load_pc1\").desc())\nload1drop_join = load1drop.join(dflrcoeff_filtered, load1drop[\"label\"] == dflrcoeff_filtered[\"label\"], 'leftouter').drop(dflrcoeff_filtered[\"label\"])\ndisplay(load1drop_join.orderBy(col(\"load_pc1\").desc()))"],"metadata":{},"outputs":[],"execution_count":129},{"cell_type":"code","source":["# Display loadings and corresponding weights from the Logistic Regression\nload2drop = load2.drop(\"load_pc1\",\"load_pc3\").orderBy(col(\"load_pc2\").desc())\nload2drop_join = load2drop.join(dflrcoeff_filtered, load2drop[\"label\"] == dflrcoeff_filtered[\"label\"], 'leftouter').drop(dflrcoeff_filtered[\"label\"])\ndisplay(load2drop_join.orderBy(col(\"load_pc2\").desc()))"],"metadata":{},"outputs":[],"execution_count":130},{"cell_type":"markdown","source":["# TESTING PERFORMANCE"],"metadata":{}},{"cell_type":"markdown","source":["We tested with the best performing (AUC and Accuracy) Random Forest model, Logistic Regression Model and Neureal Network Model."],"metadata":{}},{"cell_type":"code","source":["# Best performing random forest model\ndfrf_15_test = rf_pipeline_15.transform(testing_df)\nprint(\"Random Forest 15 trees: AUC = {}\".format(bce.evaluate(dfrf_15_test)))\nprint(dfrf_15_test.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Random Forest with 15 trees\")).show())\n# Best performing logistic regression model\ndfmodel2_pca_test = en_lr_model2_pca.transform(testing_df)\nprint(\"Logistic Regression Model 2: AUC = {}\".format(bce.evaluate(dfmodel2_pca_test)))\nprint(dfmodel2_pca_test.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Model 2\")).show())\n# Best performing neural network model\ndfnntest = mlp2_model.transform(testing_df)\nprint(\"NN Model with 3 hidden layers and 30 neurons each: Accuracy = {}\".format(evaluator.evaluate(dfnntest)))\nprint(\"NN Model with 3 hidden layers and 30 neurons each: AUC = {}\".format(evaluatorauc.evaluate(dfnntest)))"],"metadata":{},"outputs":[],"execution_count":133},{"cell_type":"markdown","source":["The AUC and accuracy increased for every model in comparison to the validation performance. Random forest seems to have the best accuracy while having the worst AUC, which should raise a red flag and would require further analyses and explanations. Neural networks perform the best overall with a huge increase in the AUC value in comparison to the other models. \n\nSo it would be worth it to really try to increase those numbers further with the optimal number of hidden layers and neurons."],"metadata":{}},{"cell_type":"markdown","source":["# Additional Analysis with all columns standardized"],"metadata":{}},{"cell_type":"markdown","source":["As mentioned in the beginning of the notebook, we noticed that logistic regression gives us \"bad\" weights for the features. We noticed that when standardizing all of the features (all dummies included, not only numerical), that we get \"better\" weights with the logistic regression model, which also make sense when combining the results with the random forest conclusions."],"metadata":{}},{"cell_type":"code","source":["training_df.columns[22:27]"],"metadata":{},"outputs":[],"execution_count":137},{"cell_type":"code","source":["training_df.columns[28:37]"],"metadata":{},"outputs":[],"execution_count":138},{"cell_type":"code","source":["# define vector assembler with the features for the modelling\nva9 = VectorAssembler(). \\\n      setInputCols(training_df.columns[22:27] + training_df.columns[28:37]). \\\n      setOutputCol('features_nonstd')\n\t  \t  \nstd9 = feature.StandardScaler(withMean=True, withStd=True).setInputCol('features_nonstd').setOutputCol('features')"],"metadata":{},"outputs":[],"execution_count":139},{"cell_type":"markdown","source":["## Random Forest 25 trees"],"metadata":{}},{"cell_type":"code","source":["# define binary classification evaluator\nbce = BinaryClassificationEvaluator()"],"metadata":{},"outputs":[],"execution_count":141},{"cell_type":"code","source":["rf9 = RandomForestClassifier(numTrees=25, maxBins=10000, featuresCol='features', labelCol='label')\nrf_pipeline_9 = Pipeline(stages=[va9, std9, rf9]).fit(training_df)\ndfrf_9 = rf_pipeline_9.transform(validation_df)\nprint(\"Random Forest 25 trees: AUC = {}\".format(bce.evaluate(dfrf_9)))\nprint(dfrf_9.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Random Forest with 25 trees\")).show())\nrfw9 = spark.createDataFrame(pd.DataFrame(list(zip(labellist, rf_pipeline_9.stages[2].featureImportances.toArray())),\n            columns = ['column', 'importancy']).sort_values('importancy').tail(20))"],"metadata":{},"outputs":[],"execution_count":142},{"cell_type":"markdown","source":["The AUC and accuracy for random forest is similiar to the one in the main part."],"metadata":{}},{"cell_type":"code","source":["display(rfw9)"],"metadata":{},"outputs":[],"execution_count":144},{"cell_type":"markdown","source":["The impoartancies also paint a similar picture."],"metadata":{}},{"cell_type":"markdown","source":["## Logistric Regression"],"metadata":{}},{"cell_type":"code","source":["lambda_par9 = 0.05\nalpha_par9 = 0.05\nen_lr9 = LogisticRegression().\\\n        setLabelCol('label').\\\n        setFeaturesCol('features').\\\n        setRegParam(lambda_par9).\\\n        setElasticNetParam(alpha_par9)"],"metadata":{},"outputs":[],"execution_count":147},{"cell_type":"code","source":["en_lr_estimator9 = Pipeline(stages=[va9, std9, en_lr9])\nen_lr_model9 = en_lr_estimator9.fit(training_df)\ndfmodel9 = en_lr_model9.transform(validation_df)\nprint(\"Logistic Regression Model: AUC = {}\".format(bce.evaluate(dfmodel9)))\t\nprint(dfmodel9.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Model\")).show())\t"],"metadata":{},"outputs":[],"execution_count":148},{"cell_type":"markdown","source":["Also in terms of logistic regression, we have a similar picture."],"metadata":{}},{"cell_type":"markdown","source":["Here the first big difference occurs, the weights with completely standardized features are way \"better\", becaue it seems that the standardized categorical features (esp. the cities) get washed out and at the same time get actually better fitted in."],"metadata":{}},{"cell_type":"code","source":["pd.DataFrame(list(zip(labellist, en_lr_model9.stages[2].coefficients.toArray())),\n            columns = ['label', 'weight']).sort_values('weight').tail(40)"],"metadata":{},"outputs":[],"execution_count":151},{"cell_type":"markdown","source":["The results tell us that it has huge impact when the company is decently established, is located in the USA, has the majority of the investors from the USA, has funding mostly by ventures and is maybe in the advertising market (category).\n\nThe negative weights show that the time to first funding should not be too high, meaning a company should attract investors fairly quick. Too many rounds also seems to have a negative impact. Seed funding seems to be negative as well, this seems to make sense because banks and venture capital investors estimate these kind of companies as risky investment, meaning the company has to esablish itself first (hihger age).\n\n<blockquote>\nWhat is 'Seed Capital'\nSeed capital is the initial capital used when starting a business, often coming from the founders' personal assets, friends or family, for covering initial operating expenses and attracting venture capitalists. This type of funding is often obtained in exchange for an equity stake in the enterprise, although with less formal contractual overhead than standard equity financing. Because banks and venture capital investors view seed capital as an \"at risk\" investment by the promoters of a new venture, capital providers may wait until a business is more established before making larger investments of venture capital funding.\n  \nSource: https://www.investopedia.com/terms/s/seedcapital.asp\n</blockquote>\n\nThis conclusion seems to be consistent with the results (importancies) of the random forest models (main and additional part)."],"metadata":{}},{"cell_type":"code","source":["pd.DataFrame(list(zip(labellist, en_lr_model9.stages[2].coefficients.toArray())),\n            columns = ['label', 'weight']).sort_values('weight').head(10)"],"metadata":{},"outputs":[],"execution_count":153},{"cell_type":"markdown","source":["## Neural Networks"],"metadata":{}},{"cell_type":"code","source":["# define evaluators for accuracy and area under the curve\nevaluator = evaluation.MulticlassClassificationEvaluator(metricName=\"accuracy\")\nevaluatorauc = evaluation.MulticlassClassificationEvaluator()\n\nmlp9 = classification.MultilayerPerceptronClassifier(seed=0).\\\n    setStepSize(0.2).\\\n    setMaxIter(200).\\\n    setFeaturesCol('features').\\\n    setLayers([2137,30,30, 2])\n\t\nmlp9_model = Pipeline(stages=[va9, std9, mlp9]).fit(training_df)\ndfnn9 = mlp9_model.transform(validation_df)\nprint(\"NN Model with 2 hidden layers and 30 neurons each: Accuracy = {}\".format(evaluator.evaluate(dfnn9)))\nprint(\"NN Model with 2 hidden layers and 30 neurons each: AUC = {}\".format(evaluatorauc.evaluate(dfnn9)))"],"metadata":{},"outputs":[],"execution_count":155},{"cell_type":"markdown","source":["The performance is a little bit worse than in the main part, but this also could be caused by the not optimal number of hidden layers and neurons."],"metadata":{}},{"cell_type":"markdown","source":["## PCA + Logistic Regression"],"metadata":{}},{"cell_type":"markdown","source":["As mentioned at the beginning, 85 PCs cover not enough variance when standardizing all the columns, which means we need to find the correct number."],"metadata":{}},{"cell_type":"code","source":["pca91 = feature.PCA(k=1500, inputCol='features', outputCol='pca_feat')\nen_lr_estimator91_pca = Pipeline(\n    stages=[va9, std9, pca91, en_lr2])\nen_lr_model91_pca = en_lr_estimator91_pca.fit(training_df)\nvarlist9 =en_lr_model91_pca.stages[2].explainedVariance\nnpvar9 = np.cumsum(varlist9[1000:1500])\n# 0.7 as starting point because we start at 1000\n# was found in other analysis\nnpvar9 = [x+0.7 for x in npvar9]\npci9 = [pci9 for pci9 in range(1000,1500,1)]\ndfvar9 = spark.createDataFrame(pd.DataFrame({\"Number of PCs\": pci9, \"Cumlative Variance Explained\": npvar9}))\ndisplay(dfvar9)"],"metadata":{},"outputs":[],"execution_count":159},{"cell_type":"markdown","source":["So we achieve 90% explained variance at around 1332 PCs, which is down to roughly 62% of the original features."],"metadata":{}},{"cell_type":"code","source":["pca9 = feature.PCA(k=1332, inputCol='features', outputCol='pca_feat')\nen_lr9_pca = LogisticRegression().\\\n        setLabelCol('label').\\\n        setFeaturesCol('pca_feat').\\\n        setRegParam(lambda_par9).\\\n        setElasticNetParam(alpha_par9)\nen_lr_estimator9_pca = Pipeline(\n    stages=[va9, std9, pca9, en_lr2])\nen_lr_model9_pca = en_lr_estimator9_pca.fit(training_df)\ndfmodel9_pca = en_lr_model9_pca.transform(validation_df)"],"metadata":{},"outputs":[],"execution_count":161},{"cell_type":"code","source":["print(\"Logistic Regression Model: AUC = {}\".format(bce.evaluate(dfmodel9_pca)))\n#\nprint(dfmodel9_pca.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Model 1\")).show())"],"metadata":{},"outputs":[],"execution_count":162},{"cell_type":"markdown","source":["Also this performance does not change that much with all columns standardized and the appropriate number of PCs."],"metadata":{}},{"cell_type":"code","source":["pc1 = en_lr_model9_pca.stages[2].pc.toArray()[:, 0].tolist()\npc2 = en_lr_model9_pca.stages[2].pc.toArray()[:, 1].tolist()\n# create pandas df with the loadings/PCs and the corresponding labels\npc_loadings = pd.DataFrame([labellist, pc1, pc2]).T.rename(columns={0: 'label', \n                                                                          1: 'load_pc1',\n                                                                          2: 'load_pc2',})"],"metadata":{},"outputs":[],"execution_count":164},{"cell_type":"code","source":["load1 = spark.createDataFrame(pd.concat((pc_loadings.sort_values('load_pc1').head(), \n           pc_loadings.sort_values('load_pc1').tail())))\ndisplay(load1)\t"],"metadata":{},"outputs":[],"execution_count":165},{"cell_type":"code","source":["# create spark df with the 5 highest and lowest loadings of the second PC\nload2 = spark.createDataFrame(pd.concat((pc_loadings.sort_values('load_pc2').head(), \n           pc_loadings.sort_values('load_pc2').tail())))\ndisplay(load2)\t"],"metadata":{},"outputs":[],"execution_count":166},{"cell_type":"markdown","source":["## Testing Performance"],"metadata":{}},{"cell_type":"code","source":["dfrf_9_test = rf_pipeline_9.transform(testing_df)\nprint(\"Random Forest 25 trees: AUC = {}\".format(bce.evaluate(dfrf_9_test)))\nprint(dfrf_9_test.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Random Forest with 25 trees\")).show())\n#\ndfmodel9_pca_test = en_lr_model9_pca.transform(testing_df)\nprint(\"Logistic Regression Model: AUC = {}\".format(bce.evaluate(dfmodel9_pca_test)))\nprint(dfmodel9_pca_test.select(fn.expr('float(label = prediction)').alias('correct')).select(fn.avg('correct').alias(\"Accuracy for Model 2\")).show())\n#\ndfnntest9 = mlp9_model.transform(testing_df)\nprint(\"NN Model with 3 hidden layers and 30 neurons each: Accuracy = {}\".format(evaluator.evaluate(dfnntest9)))\nprint(\"NN Model with 3 hidden layers and 30 neurons each: AUC = {}\".format(evaluatorauc.evaluate(dfnntest9)))"],"metadata":{},"outputs":[],"execution_count":168},{"cell_type":"markdown","source":["With all columns standardized, we come to the same conclusion.\n\nThe AUC and accuracy increased for every model in comparison to the validation performance. Random forest seems to have the best accuracy while having the worst AUC, which should raise a red flag and would require further analyses and explanations. Neural networks perform the best overall with a huge increase in the AUC value in comparison to the other models. \n\nSo it would also be worth it to really try to increase those numbers further with the optimal number of hidden layers and neurons here.\n\nHowever, the performance of the models seems to be a little worse than the performance when only standardizing the numerical features, but it is not significant."],"metadata":{}}],"metadata":{"name":"finalmodel","notebookId":3349359335119507},"nbformat":4,"nbformat_minor":0}
